{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53de4e4",
   "metadata": {},
   "source": [
    "# Keirin Prediction with GBDT\n",
    "\n",
    "このハンズオンでは勾配ブースティングライブラリであるLightGBMを使って  \n",
    "競輪の着順予測モデルを学習し、本日開催されているレースの予測を実際に作成します\n",
    "\n",
    "また、モデルは予測結果だけでなく判断根拠を求められるケースが多々あります。\n",
    "具体的には、\n",
    "\n",
    "- 予測の判断根拠自体が欲しい\n",
    "- 学習したモデルが一般的な常識/知見に基づいた特徴量を確認したい\n",
    "- システムに導入しやすくなる(納得感を与えられる)\n",
    "\n",
    "などのケースが考えられます  \n",
    "作成したモデルの判断根拠を得る手法についても紹介します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e435e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install shap\n",
    "!pip install category_encoders\n",
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from io import BytesIO\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogleCloudStorageの接続\n",
    "project_id = 'hr-mixi'\n",
    "buclet_name = 'mixi_hands_on_data_2022'\n",
    "\n",
    "client = storage.Client(project_id)\n",
    "bucket = client.get_bucket(buclet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f8379",
   "metadata": {},
   "source": [
    "### 学習用データセットの読み込み\n",
    "* 今回のデータは本研修外での利用はお控えください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'training_data/race_data.csv'\n",
    "\n",
    "blob = bucket.blob(training_data_path)\n",
    "content = blob.download_as_string()\n",
    "df = pd.read_csv(BytesIO(content))\n",
    "\n",
    "# # 着順のない選手をデータセットから取り除く\n",
    "df = df[~df['rank_number'].isnull()].reset_index(drop=True)\n",
    "df = df[df['rank_number'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a12d07",
   "metadata": {},
   "source": [
    "上記で取得したデータを観察してみましょう  \n",
    "機械学習モデルを開発する上で、データへの理解度(ドメイン知識)は重要な要素になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1701aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(c, df.iloc[0][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e43c7f",
   "metadata": {},
   "source": [
    "### ラベルデータの作成\n",
    "rank_number: 着順　を使用します\n",
    "\n",
    "着順をそのままラベルに使用するとマルチクラス分類となります    \n",
    "マルチクラス分類ではそれぞれのクラスを独立したものとして扱うが、1着と2着などを別クラスとして扱うの正しいのか考えると\n",
    "\n",
    "ラベルを`3着以内orNot`や`1着orNot`に変換する手法が考えられます\n",
    "\n",
    "\\<TODO\\>  \n",
    "your_targetに独自のラベルを作成してみてください  \n",
    "- ヒント: 着順が条件式に対してTrueであれば１, Falseであれば0に変換する例  \n",
    "`df['rank_number'].apply(lambda x: 1 if x <<条件式>> else 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849c74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encord_rank_order(rank_number):\n",
    "    # LightGBMもマルチクラス分類は0から始まるラベルが求められるが\n",
    "    # 着順は１〜９になっているため、0~8に修正する\n",
    "    return rank_number -1\n",
    "\n",
    "# 着順をそのままマルチクラス(9クラス)分類\n",
    "rank_number = df['rank_number'].apply(encord_rank_order)\n",
    "\n",
    "##  <TODO> your_targetに独自のラベルとして着順を変換する条件式を記入してください\n",
    "your_target = df['rank_number'].apply(lambda x: 1 if ______ else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47b0d2",
   "metadata": {},
   "source": [
    "### テストデータセットを分割する\n",
    "\n",
    "テストデータは日付で2021/01/01以降のレースを対象とします "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4329cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_date = 20210101\n",
    "\n",
    "train_index = df[df['event_date'] < train_test_split_date].index\n",
    "test_index = df[df['event_date'] >= train_test_split_date].index\n",
    "\n",
    "df_train = df.loc[train_index].reset_index(drop=True)\n",
    "rank_order_train = rank_number[train_index].values\n",
    "your_target_train = your_target[train_index]\n",
    "\n",
    "df_test = df.loc[test_index].reset_index(drop=True)\n",
    "rank_order_test = rank_number[test_index].values\n",
    "your_target_test = your_target[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9695bca",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "前処理では一般的に以下のような処理を行います  \n",
    "\n",
    "- 機械学習では、数値データでないと学習を行うことができないため、  文字列やカテゴリカルなデータは上手に変形する必要がある(LabelEncording, OneHotEncordingなど)  \n",
    "- アルゴリズムが学習しやすいように変形する(欠損値処理、正規化など)  \n",
    "- 組み合わせや集計処理によって新しい特徴量を作り出す  \n",
    "など\n",
    "\n",
    "前処理コードを学習と予測用で分けているのは、予測時に学習時のラベルエンコーディングのIDのマップと同じようにIDを振り分ける必要がある等\n",
    "学習時と同様のデータの変換をするように予測用の前処理コードに記載する必要があります\n",
    "\n",
    "\\<追加チャレンジ\\>\n",
    "特徴量の変形や集計で新しい特徴量を作成してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc74bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に使用しないカラムのリスト\n",
    "unuse_feature_list = [\n",
    "    'event_date', 'race_start_date_time', 'player_number', 'name', 'rank_number'\n",
    "]\n",
    "\n",
    "# レース内の集計をする特徴量\n",
    "stat_columns = [\n",
    "    'gear_ratio', 'age', 'prize', 'rank', 'total_win_count',\n",
    "    'average_race_point', 'place_rate', 'nige_count',\n",
    "    'sashi_count', 'makuri_count', 'mark_count', 'standing_count',\n",
    "    'back_count'\n",
    "]\n",
    "\n",
    "# カテゴリカル特徴量のリスト\n",
    "categorical_feature_list = [\n",
    "    'velodrome_code', 'program_name', 'grade', 'group', 'born_prefecture', 'leg_type'\n",
    "]\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "def training_preprocessing(df):\n",
    "    # カテゴリカル変数の処理\n",
    "    # LightGBMでは、ラベルエンコーディング(カテゴリをintでID付け)して、typeをcategoryに変換すればライブラリが上手く扱ってくれる\n",
    "    categorical_encorder = ce.OrdinalEncoder(cols=categorical_feature_list, handle_unknown='impute')\n",
    "    \n",
    "    df = categorical_encorder.fit_transform(df)\n",
    "    for c in categorical_feature_list:\n",
    "        df[c] = df[c].astype('category')\n",
    "        \n",
    "    # レース内での平均値の特徴量の作成\n",
    "    race_mean = df.groupby(['event_date', 'velodrome_code', 'race_number'])[stat_columns].mean()\n",
    "\n",
    "    race_mean = race_mean.rename(columns= {x: x+'_mean' for x in race_mean.columns})\n",
    "    df = pd.merge(df, race_mean, how='left', on=['event_date', 'velodrome_code', 'race_number'])\n",
    "    \n",
    "    # レース内での中央値の特徴量の作成\n",
    "    race_median = df.groupby(['event_date', 'velodrome_code', 'race_number'])[stat_columns].median()\n",
    "\n",
    "    race_median = race_median.rename(columns= {x: x+'_median' for x in race_median.columns})\n",
    "    df = pd.merge(df, race_median, how='left', on=['event_date', 'velodrome_code', 'race_number'])\n",
    "    \n",
    "    # 不要なカラムを削除する\n",
    "    df = df.drop(unuse_feature_list, axis=1)\n",
    "\n",
    "    return df, categorical_encorder\n",
    "\n",
    "\n",
    "def prediction_preprocessing(df, categorical_encorder):\n",
    "    # カテゴリカル変数の処理\n",
    "    df = categorical_encorder.transform(df)\n",
    "    for c in categorical_feature_list:\n",
    "        df[c] = df[c].astype('category')\n",
    "        \n",
    "    # レース内での平均値の特徴量の作成\n",
    "    race_mean = df.groupby(['event_date', 'velodrome_code', 'race_number'])[stat_columns].mean()\n",
    "\n",
    "    race_mean = race_mean.rename(columns= {x: x+'_mean' for x in race_mean.columns})\n",
    "    df = pd.merge(df, race_mean, how='left', on=['event_date', 'velodrome_code', 'race_number'])\n",
    "    \n",
    "    # レース内での中央値の特徴量の作成\n",
    "    race_median = df.groupby(['event_date', 'velodrome_code', 'race_number'])[stat_columns].median()\n",
    "\n",
    "    race_median = race_median.rename(columns= {x: x+'_median' for x in race_median.columns})\n",
    "    df = pd.merge(df, race_median, how='left', on=['event_date', 'velodrome_code', 'race_number'])\n",
    "    \n",
    "    # 不要なカラムを削除する\n",
    "    df = df.drop(unuse_feature_list, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9aa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproessed_df_train, categorical_encorder = training_preprocessing(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproessed_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30e269",
   "metadata": {},
   "source": [
    "### 学習と検証データを分割する\n",
    "\n",
    "学習用と検証用は0.8:0.2で分割しています  \n",
    "検証用データは、モデル学習時のtest_lossの計算のほか  \n",
    "パラメーターの自動チューニング時のloss指標の計算に使用されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rate = 0.2\n",
    "\n",
    "train_val_split_point = int(len(df_train)*(1-val_rate))\n",
    "\n",
    "df_val = preproessed_df_train.iloc[train_val_split_point:].reset_index(drop=True)\n",
    "df_train = preproessed_df_train.iloc[:train_val_split_point].reset_index(drop=True)\n",
    "\n",
    "rank_order_val = rank_order_train[train_val_split_point:]\n",
    "rank_order_train = rank_order_train[:train_val_split_point]\n",
    "\n",
    "your_target_val = your_target_train[train_val_split_point:]\n",
    "your_target_train = your_target_train[:train_val_split_point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd24e3",
   "metadata": {},
   "source": [
    "### モデルの学習\n",
    "\n",
    "LightGBMのパラメータ\n",
    "- 出力形式に関する要素\n",
    "    - objective\n",
    "        - regression: 回帰\n",
    "        - binary: 二値分類\n",
    "        - multiclass: マルチクラス分類\n",
    "    - metric\n",
    "        - 回帰\n",
    "            - mae: mean absolute error・平均絶対誤差\n",
    "            - mse: mean squared error平均2乗誤差\n",
    "        - 二値分類\n",
    "            - binary_logloss:　クロスエントロピー\n",
    "            - binary_error: 正解率\n",
    "        - マルチクラス分類\n",
    "            - multi_logloss: softmax\n",
    "            - multi_error: 正解率\n",
    "- モデル構造に関する要素\n",
    "    - learning_rate: 学習率 Default=0.1 0以上\n",
    "    - num_iterations: 木の数\n",
    "    - num_leaves: 葉(条件の数)\n",
    "    \n",
    "    等, \n",
    "    公式リファレンス　https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "\n",
    "\\<TODO\\>  \n",
    "lgb_paramsの`objective`と`metric`を正しい値で埋めてください\n",
    "  \n",
    "\\<追加チャレンジ\\> \n",
    "lgb_paramsに任意のパラメータを追加して、モデルの精度の変化を観察してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac248de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 着順のマルチクラス分類モデルの学習\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train, rank_order_train)\n",
    "lgb_test = lgb.Dataset(df_val, rank_order_val, reference=lgb_train)\n",
    "\n",
    "## <TODO> lgb_paramsのobjectiveとmetricを正しい値で埋めてください\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 1000,\n",
    "    'num_class': 9,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': '_____',\n",
    "    'metric': '_____',\n",
    "}\n",
    "\n",
    "booster_rank_number = lgb.train(\n",
    "    lgb_params, lgb_train, valid_sets=lgb_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812b417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# オリジナルターゲット（2値分類）のモデルの学習\n",
    "lgb_train = lgb.Dataset(df_train, your_target_train)\n",
    "lgb_test = lgb.Dataset(df_val, your_target_val, reference=lgb_train)\n",
    "\n",
    "## <TODO> lgb_paramsのobjectiveとmetricを正しい値で埋めてください\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': '_____',\n",
    "    'metric': '_____',\n",
    "}\n",
    "\n",
    "booster_your_target = lgb.train(\n",
    "    lgb_params, lgb_train, valid_sets=lgb_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe44d1",
   "metadata": {},
   "source": [
    "### モデルの判断根拠1 Importance\n",
    "feature_importanceメソッドでモデルにおける特徴量の重要度を得られます  \n",
    "重要度の指標:\n",
    "- split: 決定木の分岐を使用した数  \n",
    "- gain: その特徴量が使用する分岐からの目的関数の減少  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの前処理の実行\n",
    "preprocessed_df_test = prediction_preprocessing(df_test, categorical_encorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8ac45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split\n",
    "importance = pd.DataFrame(\n",
    "    booster_your_target.feature_importance(importance_type = 'split'),\n",
    "    index=preproessed_df_train.columns,\n",
    "    columns=['importance']\n",
    ")\n",
    "importance.sort_values(['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781abb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gain\n",
    "importance = pd.DataFrame(\n",
    "    booster_your_target.feature_importance(importance_type = 'gain'),\n",
    "    index=preproessed_df_train.columns,\n",
    "    columns=['importance']\n",
    ")\n",
    "importance.sort_values(['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d878279",
   "metadata": {},
   "source": [
    "### モデルの判断根拠1 shap\n",
    "SHAPというライブラリを用いて、特徴量がモデル/予測に対してどのような影響を与えたかを計測できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d81c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBMは決定木アルゴリズムなのでTreeExplainerを使う\n",
    "# NNモデルでDeepExplainerを使うと今回のようなテーブルデータだけでなく、画像データに対しても適用することができる\n",
    "\n",
    "explainer = shap.TreeExplainer(booster_your_target, feature_perturbation = \"tree_path_dependent\")\n",
    "shap_values = explainer.shap_values(preprocessed_df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の貢献度をプロット\n",
    "shap.summary_plot(shap_values, preprocessed_df_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a895c5",
   "metadata": {},
   "source": [
    "### 予測ごとの判断根拠\n",
    "shap_valuesには各予測に対しての各特徴量の貢献度が格納されている  \n",
    "これを使うことで、この住宅は[根拠]だからYという表現ができる  \n",
    "\n",
    "検証用データの0番目の予測結果を見てみましょう  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "shap_v = pd.DataFrame(shap_values[1], columns=preprocessed_df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5088e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果に対しでpositiveな影響を与えている特徴量\n",
    "shap_v.iloc[idx].sort_values(ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果に対しでnegativeな影響を与えている特徴量\n",
    "shap_v.iloc[idx].sort_values(ascending=True).iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455da7b5",
   "metadata": {},
   "source": [
    "### 精度検証\n",
    "今回精度の指標は、  \n",
    "`レース内で最も1着である確率である確率が高いと評価された選手が実際に1着であった確率`  \n",
    "と設定します\n",
    "\n",
    "データは１行で1選手を表現しているため、1レース単位は複数行を集約する必要があります  \n",
    "groupbyを使ってレースごとのindexを取得しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_groups = df_test.groupby(['event_date', 'velodrome_code', 'race_number']).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87325e20",
   "metadata": {},
   "source": [
    "### 着順のマルチクラス分類モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f283cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の取得\n",
    "rank_number_prediction_result = booster_rank_number.predict(preprocessed_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の0番目の出力\n",
    "# 9つの要素がありそれぞれが各ラベルの確率を表現しています、つまり合計は1になります\n",
    "print(rank_number_prediction_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レースごとの出力値の0番目(1着と推定した確率)がもっとも大きい(np.argmax)選手の実際の着順が1着だった数/ レースの数\n",
    "[\n",
    "    df_test.iloc[idx[np.argmax(\n",
    "        [\n",
    "            res[0] for res in rank_number_prediction_result[idx]\n",
    "        ]\n",
    "    )]]['rank_number']\n",
    "    for race, idx in race_groups.items()\n",
    "].count(1) / len(race_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b96eb",
   "metadata": {},
   "source": [
    "### オリジナルラベルでのモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_target_prediction_result = booster_your_target.predict(preprocessed_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_target_prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61781e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# レースごとの出力値がもっとも大きい(np.argmax)選手の実際の着順が1着だった数/ レースの数\n",
    "[\n",
    "    df_test.iloc[idx[np.argmax(your_target_prediction_result[idx])]]['rank_number']\n",
    "    for race, idx in race_groups.items()\n",
    "].count(1) / len(race_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803da971",
   "metadata": {},
   "source": [
    "### 今日の予測結果の作成\n",
    "\n",
    "GCSに本日のレースデータのcsvを作成してあります。  \n",
    "このデータに対する予測結果を作成し実際のレース結果を見比べてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本日のレースデータをGCSから取得\n",
    "prediction_data_path = 'prediction_data/race_data.csv'\n",
    "blob = bucket.blob(prediction_data_path)\n",
    "content = blob.download_as_string()\n",
    "prediction_df = pd.read_csv(BytesIO(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3324666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupsの作成\n",
    "prediction_race_groups = prediction_df.groupby(['event_date', 'velodrome_code', 'race_number']).groups\n",
    "\n",
    "# 前処理の実行\n",
    "preprocessed_prediction_df = prediction_preprocessing(prediction_df, categorical_encorder)\n",
    "\n",
    "# 予測結果の作成\n",
    "today_prediction_result = booster_your_target.predict(preprocessed_prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f796a7",
   "metadata": {},
   "source": [
    "### 予測結果の確認\n",
    "本日開催中のレースデータでの予測なので、着順データ用意していません(できない)。  \n",
    "Tipstarで任意のレースの予測結果を確認しましょう  \n",
    "https://tipstar.com/keirin/channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "velodrome_code = {\n",
    "    11: '函館競輪場', \n",
    "    12: '青森競輪場', \n",
    "    13: 'いわき平競輪場', \n",
    "    21: '弥彦競輪場', \n",
    "    22: '前橋競輪場', \n",
    "    23: '取手競輪場', \n",
    "    24: '宇都宮競輪場', \n",
    "    25: '大宮競輪場', \n",
    "    26: '西武園競輪場', \n",
    "    27: '京王閣競輪場', \n",
    "    28: '立川競輪場', \n",
    "    31: '松戸競輪場', \n",
    "    32: '千葉競輪場', \n",
    "    33: '花月園競輪場', \n",
    "    34: '川崎競輪場', \n",
    "    35: '平塚競輪場', \n",
    "    36: '小田原競輪場', \n",
    "    37: '伊東競輪場', \n",
    "    38: '静岡競輪場', \n",
    "    41: '一宮競輪場', \n",
    "    42: '名古屋競輪場', \n",
    "    43: '岐阜競輪場', \n",
    "    44: '大垣競輪場', \n",
    "    45: '豊橋競輪場', \n",
    "    46: '富山競輪場', \n",
    "    47: '松阪競輪場', \n",
    "    48: '四日市競輪場', \n",
    "    51: '福井競輪場', \n",
    "    52: '大津競輪場', \n",
    "    53: '奈良競輪場', \n",
    "    54: '向日町競輪場', \n",
    "    55: '和歌山競輪場', \n",
    "    56: '岸和田競輪場', \n",
    "    61: '玉野競輪場', \n",
    "    62: '広島競輪場', \n",
    "    63: '防府競輪場', \n",
    "    71: '高松競輪場', \n",
    "    72: '観音寺競輪場', \n",
    "    73: '小松島競輪場', \n",
    "    74: '高知競輪場', \n",
    "    75: '松山競輪場', \n",
    "    81: '小倉競輪場', \n",
    "    83: '久留米競輪場', \n",
    "    84: '武雄競輪場', \n",
    "    85: '佐世保競輪場', \n",
    "    86: '別府競輪場',\n",
    "    87: '熊本競輪場', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac717022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for race, idx in prediction_race_groups.items():\n",
    "    print('会場: ', velodrome_code[race[1]])\n",
    "    print('レース番号: ', race[2])\n",
    "    \n",
    "    # 1レース分の予測結果を正規化(合計1に変換)して確率の形にしています\n",
    "    normed_result = today_prediction_result[idx] / sum(today_prediction_result[idx])\n",
    "    \n",
    "    for entry_num, prob in zip(prediction_df.iloc[idx]['entry_number'].values, normed_result):\n",
    "        print(entry_num, '番: ', round(prob, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cbe66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df23d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
